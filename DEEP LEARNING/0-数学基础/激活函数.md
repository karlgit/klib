本文主要介绍常见的激活函数的特征和区别，以及使用matplot绘制。
实现的激活函数包括tanh、sigmoid、softmax、relu等。

## 激活函数概念
激活函数的主要作用是提供网络的非线性建模能力。
在我们面对线性可分的数据集的时候，简单的用线性分类器即可解决分类问题。但是现实生活中的数据往往不是线性可分的，面对这样的数据，一般有两个方法：引入非线性函数、线性变换。
**线性变换**，就是把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。
**激活函数**，在神经网络中，为了避免单纯的线性组合，我们在每一层的输出后面都添加一个激活函数。
　
1. sigmoid函数
> sigmoid函数： $y=sigmoid(x)=\frac {1}{1+e^{-x}}$ 
> 
> 存在问题：
> 
> 1.输出范围在0~1之间，均值为0.5，需要做数据偏移，不方便下一层的学习。
> 
> 2.当x很小或很大时，导数很小会导致网络无法有效训练，这就是所谓的梯度消失。神经网络训练使用的是反向传播算法（主要利用的是导数的链式法则，也就是多个导数的乘积），多个导数相乘很容易趋于0。
> 
![](https://img-blog.csdnimg.cn/20200809121325596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6ajUwMDAyODAx,size_16,color_FFFFFF,t_70)
- tanh函数
> tanh函数：$y=tanh(x)=\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}$ 
> 
> 优点：输出在-1~1之间，均值为0，收敛速度比sigmoid快。
> 
> 存在问题：在函数左右两侧梯度接近0，会导致梯度消失，导致网络无法有效的训练。
> 

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080912131560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6ajUwMDAyODAx,size_16,color_FFFFFF,t_70)

- relu函数
> relu函数：
> $$
        f(x) =
        \begin{cases}
        x,  & \text{if $x\geq0$} \\
        0, & \text{if $x\leq0$}
        \end{cases}
$$
> ReLU的全称是Rectified Linear Units。
> 当x<0时，ReLU硬饱和，当x>0时，则不存在饱和问题。
> 
> 存在问题：随着训练的推进，部分输入会落入硬饱和区，导致对应权重无法更新。这种现象被称为“神经元死亡”。
>  
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200809121303460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6ajUwMDAyODAx,size_16,color_FFFFFF,t_70)


- leakly relu函数
> leakly relu函数：
> $$
        f(x) =
        \begin{cases}
        x,  & \text{if $x\geq0$} \\
        \alpha(e^x-1), & \text{if $x\leq0$}
        \end{cases}
$$
> 优点: 解决了relu神经元死亡的问题，这里原文建议$\alpha$=0.25

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200809125031197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6ajUwMDAyODAx,size_16,color_FFFFFF,t_70)



**//TODO 还有很多激活函数待补充**
<br/>
<br/>
<br/>
<br/>

- softmax函数
> softmax函数：$y=\frac{e^x}{\sum{e^x}}$
> 
> 特征：总的概率值为1，用于多分类任务，输出值为每一项的概率。
> 
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200809121340531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x6ajUwMDAyODAx,size_16,color_FFFFFF,t_70)

<br/>
<br/>


## 代码实现
```python
import numpy as np 
from matplotlib import pyplot as plt 

def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

def sigmoid(x):
    return 1. / (1 + np.exp(-x))

def relu(x):
    return np.where(x<0,0,x)

def leaklyrelu(x,a=0.2):
	return x*np.clip((x>=0),a,1)

def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)

def show(title,x,y): 
    plt.title(title) 
    plt.xlabel("x") 
    plt.ylabel("y") 
    plt.plot(x,y) 
    plt.show()
    
x = np.arange(-6,6,0.1) 
y = tanh(x) 
show("tanh",x,tanh(x))
show("sigmoid",x,sigmoid(x))
show("softmax",x,softmax(x))
show("relu",x,relu(x))
show("relu",x,relu(x))
```

参考文献：
https://www.cnblogs.com/missidiot/p/9378079.html